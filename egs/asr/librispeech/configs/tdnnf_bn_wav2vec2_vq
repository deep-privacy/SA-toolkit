[cmd]
cpu_cmd = ./utils/run.pl
cuda_cmd = ./utils/run.pl

# Can be modified through venv: e.g.: vq=128 local/chain/train.py ...
[var]
vq = 48
# dset = train_clean_100
dset = train_600

[exp]
train_set = data/${:dset}_sp
tree_dir = exp/chain/e2e_${:dset}/e2e_biphone_tree
egs_dir = exp/chain/e2e_${:dset}/fst_egs

# train params
num_epochs = 4
num_jobs_initial = 2
num_jobs_final = 5
lr_initial = 0.0001
lr_final =   0.000001
diagnostics_interval = 20
checkpoint_interval = 5
minibatch_size = 16
# grad_acc_steps = 16
train_stage = last

xent_regularize = 0.02
weight_decay_l2_regularize_factor = 0.01

final_combination_n_model = 1

max_concurrent_jobs = 2

model_file = local/chain/tuning/tdnnf_wav2vec2_vq.py
dirname = bn_tdnnf_wav2vec2_${:dset}_vq_${:vq}
model_args = ["--freeze-encoder", "True", "--codebook-size", "${:vq}"]
init_weight_model = "./exp/chain/bn_tdnnf_wav2vec2_${:dset}_aug/final.pt"

# a=bn_tdnnf_wav2vec2_vq_; tail $a_*/decode_test_clean*final_fg/scoringDetails/best_wer | grep "WER" | awk '{print $1 "\t"$2 "\t" $14}' | cut -d/ -f1,3 | sort -k4,4 -n -t"_"


[test]
graph_dir = exp/chain/e2e_${:dset}/e2e_biphone_tree/graph_tgsmall

# test_set = data/test_clean
test_set = data/test_clean,data/test_other
# test_set = data/test_other
# test_set = data/dev_other
# test_set = data/dev_clean

# decode on gpus
num_jobs = 9
gpu = True

# vim:set et sw=2 ts=2 ft=toml:
