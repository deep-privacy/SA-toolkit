# Copyright (c) 2020 Idiap Research Institute, http://www.idiap.ch/
#  Written by Srikanth Madikeri <srikanth.madikeri@idiap.ch>

import sys
import os
import random
from collections import OrderedDict, Counter
import logging
import argparse
from dataclasses import dataclass
import torch
import torch.nn as nn
from torch.nn.utils import clip_grad_value_
import torch.optim as optim
from _pkwrap import kaldi
from .. import script_utils
from .. import utils
from collections import defaultdict, namedtuple
import subprocess
import io
from math import ceil
from .egs import prepare_minibatch
from .egs_wav2vec2 import Wav2vec2BatchSampler, Wav2vec2EgsCollectFn, GetSupervisionFromWav2Vec2Egs

import damped

class KaldiChainObjfFunction(torch.autograd.Function):
    """LF-MMI objective function for pytorch

    This Function wraps MMI loss function implemented in Kaldi.
    See Pytorch documentation of how to extend Function to check
    the attributes present. I expect that this class will be used
    as follows

    ```
        lfmmi_loss = KaldiChainObjfFunction.apply
        ...
        lfmmi_loss(chain_opts, den_graph, egs, nnet_output, xent_output)
    ```
    """
    @staticmethod
    @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
    def forward(ctx, opts, den_graph, supervision, nnet_output_tensor,
                xent_out_tensor):
        """This function computes the loss for a single minibatch.

        This function calls Kaldi's ComputeChainObjfAndDeriv through our
        pybind11 wrapper. It takes the network outputs, rearranges them
        in the way Kaldi expects, gets back the derivates of the outputs.
        We pre-allocate the space for derivatives before passing to Kaldi.
        No extra space is used by Kaldi as we pass only the poitners.

        Args:
            opts: training options for the loss function
            den_graph: Denominator graph
            supervision: merged egs for the current minibatch
            nnet_output_tensor: output generated by the network
            xent_out_tensor: the corresponding cross-entropy output

        Returns:
            We normally don't use the output returned by the function.
            The derivatives are stored in the context and used by the backward()
            function.
        """
        objf = torch.zeros(1, requires_grad=False)
        l2_term = torch.zeros(1, requires_grad=False)
        weight = torch.zeros(1, requires_grad=False)
        mb, T, D = nnet_output_tensor.shape
        # Kaldi expects the outputs to be groups by time frames. So
        # we need to permut the output
        nnet_output_copy = nnet_output_tensor.permute(1, 0, 2).reshape(-1, D).contiguous()
        nnet_deriv = torch.zeros_like(nnet_output_copy).contiguous()
        if xent_out_tensor is not None:
            xent_deriv = torch.zeros_like(nnet_output_copy).contiguous()
            kaldi.chain.ComputeChainObjfAndDeriv(
                opts,
                den_graph,
                supervision,
                nnet_output_copy,
                objf,
                l2_term,
                weight,
                nnet_deriv,
                xent_deriv,
            )
            nnet_deriv = nnet_deriv.reshape(T, mb, D).permute(1, 0, 2).contiguous()
            xent_deriv = xent_deriv.reshape(T, mb, D).permute(1, 0, 2).contiguous()
            xent_objf = (xent_out_tensor*xent_deriv).sum()/(mb*T)
            objf[0] = objf[0]/weight[0]
            logging.info(
                "objf={}, l2={}, xent_objf={}".format(
                    objf[0],
                    l2_term[0]/weight[0],
                    xent_objf,
                )
            )
            ctx.save_for_backward(nnet_deriv, xent_deriv, torch.tensor(opts.xent_regularize, requires_grad=False))
        else:
            kaldi.chain.ComputeChainObjfAndDerivNoXent(
                opts,
                den_graph,
                supervision,
                nnet_output_copy,
                objf,
                l2_term,
                weight,
                nnet_deriv,
            )
            nnet_deriv = nnet_deriv.reshape(T, mb, D).permute(1, 0, 2).contiguous()
            xent_deriv = None
            objf[0] = objf[0]/weight[0]
            logging.info(
                "objf={}, l2={}".format(
                    objf[0],
                    l2_term[0]/weight[0],
                )
            )
            ctx.save_for_backward(nnet_deriv)
        # return the derivates in the original order
        return objf

    @staticmethod
    @torch.cuda.amp.custom_bwd
    def backward(ctx, dummy):
        """returns the derivatives"""
        if len(ctx.saved_tensors) == 3:
            nnet_deriv, xent_deriv, xent_regularize = ctx.saved_tensors
            return None, None, None, -nnet_deriv, -xent_regularize*xent_deriv
        else:
            nnet_deriv = ctx.saved_tensors[0]
            return None, None, None, -nnet_deriv, None

class OnlineNaturalGradient(torch.autograd.Function):
    """A wrapper to NG-SGD class in Kaldi

    This class wraps Natural Gradient implemented in Kaldi by calling
    nnet3's precondition_directions (wrapped through pybind11)
    When implemented as an autograd Function we can easily wrap
    it in a Linear layer. See pkwrap.nn.NaturalAffineTransform.
    """
    @staticmethod
    @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)
    def forward(ctx, input, weight, bias, in_state, out_state):
        """Forward pass for NG-SGD layer

        Args:
            input: the input to the layer (a Tensor)
            weight: weight matrix of the layer (a Tensor)
            bias: the bias parameters of the layer (a Tensor)
            in_state: state of the input (a kaldi.nnet3.OnlineNaturalGradient object)
            out_state: state of the output (a kaldi.nnet3.OnlineNaturalGradient object)

        Returns:
            Linear transformation of the input with weight and bias.
            The other inputs are saved in the context to be used during the call
            to backward.
        """
        ctx.save_for_backward(input, weight, bias)
        ctx.states = [in_state, out_state]
        # the code below is based on pytorch's F.linear
        if input.dim() == 2 and bias is not None:
            output = torch.addmm(bias, input, weight.t())
        else:
            output = input.matmul(weight.t())
            if bias is not None:
                output += bias
        return output

    @staticmethod
    @torch.no_grad()
    @torch.cuda.amp.custom_bwd
    def backward(ctx, grad_output):
        """Backward pass for NG-SGD layer

        We pass the gradients computed by Pytorch to Kaldi's precondition_directions
        given the states of the layer.
        """
        input, weight, _ = ctx.saved_tensors
        in_state, out_state = ctx.states
        if input.dim() == 3:
            mb, T, D = input.shape
            mb_T = mb*T
        else:
            mb_T, D = input.shape
        input_temp = torch.zeros(mb_T, D+1, device=input.device, requires_grad=False).contiguous()
        input_temp[:,-1] = 1.0
        input_temp[:,:-1].copy_(input.reshape(mb_T, D))
        grad_weight = grad_bias = None
        if grad_output.dim() == 3:
            grad_input = grad_output.matmul(weight)
            grad_input = grad_input.reshape(mb, T, D)
        else:
            grad_input = grad_output.mm(weight)
        in_scale = kaldi.nnet3.precondition_directions(in_state, input_temp)
        out_dim = grad_output.shape[-1]
        grad_output_temp = grad_output.view(-1, out_dim)
        out_scale = kaldi.nnet3.precondition_directions(out_state, grad_output_temp) # hope grad_output is continguous!
        scale = in_scale*out_scale
        grad_output.data.mul_(scale)
        # TODO: check if we should use data member instead?
        grad_weight = grad_output_temp.t().mm(input_temp[:,:-1])
        grad_bias = grad_output_temp.t().mm(input_temp[:,-1].reshape(-1,1))
        grad_weight.data.mul_(scale)
        grad_bias.data.mul_(scale)
        return grad_input, grad_weight, grad_bias.t(), None, None

def train_lfmmi_one_iter(model, dataset, den_fst_path, training_opts,
                         minibatch_size=16, lr=0.0001,
                         weight_decay=0.25, frame_shift=0,
                         print_interval=30,
                         frame_subsampling_factor=3,
                         tensorboard = None,
                         optimizer = None,
                         e2e = False,
    ):
    """Run one iteration of LF-MMI training

    The function loads the latest model, takes a list of egs, path to denominator
    fst and runs through the merged egs for one iteration of training. This is
    similar to how one iteration of training is completed in Kaldi.

    Args:
        model: Path to pytorch model (.pt file)
        dataset: a Wav2vec2EgsDataset dataset
        den_fst_path: path to den.fst file
        training_opts: options of type ChainTrainingOpts
        minibatch_size: 
        lr: learning rate
        frame_shift: an integer (usually 0, 1, or 2) used to shift the training features
        print_interval: the interval (a positive integer) to print the loss value

    Returns:
        updated model in CPU
    """

    if hasattr(model, 'vq') and model.vq():
        logging.info("USING ADDITIONAL VQ commitment loss")

    # this is required to make sure Kaldi uses GPU
    kaldi.InstantiateKaldiCuda()
    if training_opts is None:
        training_opts = kaldi.chain.CreateChainTrainingOptionsDefault()
    den_graph = kaldi.chain.LoadDenominatorGraph(den_fst_path, model.output_dim)
    criterion = KaldiChainObjfFunction.apply
    model = model.cuda()
    if optimizer is None:
        optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)
    acc_sum = torch.tensor(0., requires_grad=False)
    acc_sum_vq = torch.tensor(0., requires_grad=False)
    acc_sum_perplexity = torch.tensor(0., requires_grad=False)
    #  scaler = torch.cuda.amp.GradScaler()

    spk_branch = damped.disturb.DomainTask(name="speaker_identificaion", to_rank=0)

    batch_sampler = Wav2vec2BatchSampler(
        dataset.egs_holder,
        batch_size=minibatch_size,
        drop_last=False,
    )
    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, collate_fn=Wav2vec2EgsCollectFn, num_workers=8)

    #  for mb_id, data in enumerate(dataloader):
        #  print(mb_id, data[0].shape, GetSupervisionFromWav2Vec2Egs(dataset.transition_model, dataset.normalization_fst, data[1], 500), flush=True)
    #  sys.exit(0)

    for mb_id, data in enumerate(dataloader):
        features = data[0].cuda()

        output, xent_output = model(features)
        #  print("OUT:", output.shape, flush=True)

        # pchampio send the hidden state to domain task (async)
        uttid_list = list(map(lambda x: x.name, data[1]))
        #  print(uttid_list, flush=True)
        uttid_list = list(map(lambda x: damped.utils.str_int_encoder.encode(x), uttid_list))
        #  print(uttid_list, flush=True)
        req = spk_branch.fork_detach(model.bottleneck_out.detach().cpu(),
                               torch.tensor(uttid_list, dtype=torch.long),
                               dtype=(torch.float32, torch.long))

        num_output_frames = output.shape[1]
        sup = GetSupervisionFromWav2Vec2Egs(dataset.transition_model, dataset.normalization_fst, data[1], num_output_frames)
        deriv = criterion(training_opts, den_graph, sup, output, xent_output)
        
        acc_sum.add_(deriv[0])
        if mb_id>0 and mb_id%print_interval==0:
            logging.info("Overall objf={}".format(acc_sum/print_interval))
            if tensorboard: tensorboard.add_scalar('ASR_objf/train', acc_sum/print_interval, mb_id)
            acc_sum.zero_()

        if hasattr(model, 'vq') and model.vq():
            acc_sum_vq.add_(model.vq_loss.item())
            if mb_id>0 and mb_id%print_interval==0:
                logging.info("Overall VQ objf={}".format(acc_sum_vq/print_interval))
                if tensorboard: tensorboard.add_scalar('VQ_objf/train', acc_sum_vq/print_interval, mb_id)
                acc_sum_vq.zero_()
            deriv += model.vq_loss.to(deriv.device) # add to main loss

        if hasattr(model, 'vq') and model.vq() and hasattr(model, 'perplexity'):
            acc_sum_perplexity.add_(model.perplexity.item())
            if mb_id>0 and mb_id%print_interval==0:
                logging.info("VQ perplexity ={}".format(acc_sum_perplexity/print_interval))
                if tensorboard: tensorboard.add_scalar('VQ_perplexity/train', acc_sum_perplexity/print_interval, mb_id)
                acc_sum_perplexity.zero_()

        if hasattr(model, 'additional_obj'):
            model.additional_obj(deriv,
                                 should_log=mb_id>0 and mb_id%print_interval==0,
                                 print_interval=print_interval,
                                 tensorboard=tensorboard)

        #  TODO: move this to nn.models
        #  def additional_obj(self, deriv, should_log=False, print_interval=1, tensorboard=None):
            #  if deriv == None or self.wav2vec2_loss:
                #  return
            #  deriv += self.wav2vec2_loss.to(deriv.device)

            #  if should_log:
                #  logging.info("Wav2Vec2 loss ={}".format(self.acc_sum_wav2vec2_loss/print_interval))
                #  if tensorboard: tensorboard.add_scalar('wav2vec2/train', self.acc_sum_wav2vec2_loss/print_interval, mb_id)
                #  self.acc_sum_wav2vec2_loss.zero_()


        optimizer.zero_grad()
        #  scaler.scale(deriv.cuda()).backward()
        deriv.backward()
        #  scaler.unscale_(optimizer)
        clip_grad_value_(model.parameters(), 5.0)
        #  scaler.step(optimizer)
        #  scaler.update()
        optimizer.step()

        # pchampio wait for domain branch to fully have received the hidden state
        req.wait()

        #  return model # fast_test
    model = model.cpu()
    if tensorboard: tensorboard.close()
    return model

@torch.no_grad()
def compute_chain_objf(model, dataset, den_fst_path, training_opts,
    minibatch_size=16, frame_shift=0,
    frame_subsampling_factor=3,
    tensorboard = None
    ):
    """Function to compute objective value from a minibatch, useful for diagnositcs.

    Args:
        model: the model to run validation on
        dataset: a Wav2vec2EgsDataset dataset
        den_fst_path: path to den.fst
        training_opts: ChainTrainingOpts object
        frame_subsampling_factor: subsampling to be used on the output
    """
    if training_opts is None:
        training_opts = kaldi.chain.CreateChainTrainingOptionsDefault()
    den_graph = kaldi.chain.LoadDenominatorGraph(den_fst_path, model.output_dim)
    criterion = KaldiChainObjfFunction.apply
    model = model.cuda()
    acc_sum = torch.tensor(0., requires_grad=False)
    acc_sum_vq = torch.tensor(0., requires_grad=False)
    acc_sum_perplexity = torch.tensor(0., requires_grad=False)
    tot_weight = 0.

    if torch.distributed.is_initialized():
        spk_branch = damped.disturb.DomainTask(name="speaker_identificaion", to_rank=0)

    batch_sampler = Wav2vec2BatchSampler(
        dataset.egs_holder,
        batch_size=minibatch_size,
        drop_last=False,
    )
    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler, collate_fn=Wav2vec2EgsCollectFn, num_workers=4)

    for mb_id, data in enumerate(dataloader):
        features = data[0].cuda()

        output, xent_output = model(features)

        # pchampio send the hidden state to domain task (async)
        if torch.distributed.is_initialized():
            uttid_list = list(map(lambda x: x.name, data[1]))
            uttid_list = list(map(lambda x: damped.utils.str_int_encoder.encode(x), uttid_list))

            req = spk_branch.fork_detach(model.bottleneck_out.detach().cpu(),
                                   torch.tensor(uttid_list, dtype=torch.long),
                                   dtype=(torch.float32, torch.long))

        num_output_frames = output.shape[1]
        sup = GetSupervisionFromWav2Vec2Egs(dataset.transition_model, dataset.normalization_fst, data[1], num_output_frames)
        deriv = criterion(training_opts, den_graph, sup, output, xent_output)

        mb, num_seq = features.shape
        tot_weight += mb*num_seq
        acc_sum.add_(deriv[0]*mb*num_seq)

        if hasattr(model, 'vq') and model.vq():
            acc_sum_vq.add_(model.vq_loss.item()*mb*num_seq)

        if hasattr(model, 'vq') and model.vq() and hasattr(model, 'perplexity'):
            acc_sum_perplexity.add_(model.perplexity.item()*mb*num_seq)

        # pchampio wait for domain branch to fully have received the hidden state
        if torch.distributed.is_initialized():
            req.wait()

    objf = acc_sum/tot_weight
    logging.info("Objective = {}".format(objf))
    if tensorboard: tensorboard.add_scalar('ASR_objf/valid', objf, 1)
    if tensorboard: tensorboard.close()

    if hasattr(model, 'vq') and model.vq():
        logging.info("Overall VQ objf={}".format(acc_sum_vq/tot_weight))
        if tensorboard: tensorboard.add_scalar('VQ_objf/valid', acc_sum_vq/tot_weight, 1)

    if hasattr(model, 'vq') and model.vq() and hasattr(model, 'perplexity'):
        logging.info("Overall perplexity={}".format(acc_sum_perplexity/tot_weight))
        if tensorboard: tensorboard.add_scalar('VQ_perplexity/valid', acc_sum_perplexity/tot_weight, 1)

    model = model.cpu()
    return model, objf

