{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fJUJLWQ92g6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a9e158-dacc-49d4-8edc-b0dddefd80eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.7/318.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e200MmBU2aLT",
        "outputId": "cd0a21c0-c079-4a04-88ec-8e62462bd936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://269cfeb349812120fa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://269cfeb349812120fa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "def convert_to_16_bit_wav(data):\n",
        "    # Based on: https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html\n",
        "    #breakpoint()\n",
        "    if data.dtype == np.float32:\n",
        "        print(\n",
        "            \"Audio data is not in 16-bit integer format.\",\n",
        "            \"Trying to convert to 16-bit int format.\",\n",
        "            file=sys.stderr\n",
        "        )\n",
        "        data = data / np.abs(data).max()\n",
        "        data = data * 32767\n",
        "        data = data.astype(np.int16)\n",
        "    elif data.dtype == np.int32:\n",
        "        print(\n",
        "            \"Audio data is not in 16-bit integer format.\",\n",
        "            \"Trying to convert to 16-bit int format.\",\n",
        "            file=sys.stderr\n",
        "        )\n",
        "        data = data / 65538\n",
        "        data = data.astype(np.int16)\n",
        "    elif data.dtype == np.int16:\n",
        "        pass\n",
        "    elif data.dtype == np.uint8:\n",
        "        print(\n",
        "            \"Audio data is not in 16-bit integer format.\",\n",
        "            \"Trying to convert to 16-bit int format.\",\n",
        "            file=sys.stderr\n",
        "        )\n",
        "        data = data * 257 - 32768\n",
        "        data = data.astype(np.int16)\n",
        "    else:\n",
        "        raise ValueError(\"Audio data cannot be converted to \" \"16-bit int format.\")\n",
        "    return data\n",
        "\n",
        "def pcm2float(sig, dtype='float32'):\n",
        "    \"\"\"\n",
        "    https://gist.github.com/HudsonHuang/fbdf8e9af7993fe2a91620d3fb86a182\n",
        "    \"\"\"\n",
        "    sig = np.asarray(sig)\n",
        "    if sig.dtype.kind not in 'iu':\n",
        "        raise TypeError(\"'sig' must be an array of integers\")\n",
        "    dtype = np.dtype(dtype)\n",
        "    if dtype.kind != 'f':\n",
        "        raise TypeError(\"'dtype' must be a floating point type\")\n",
        "\n",
        "    i = np.iinfo(sig.dtype)\n",
        "    abs_max = 2 ** (i.bits - 1)\n",
        "    offset = i.min + abs_max\n",
        "    return (sig.astype(dtype) - offset) / abs_max\n",
        "\n",
        "\n",
        "def float2pcm(sig, dtype='int16'):\n",
        "    \"\"\"\n",
        "    https://gist.github.com/HudsonHuang/fbdf8e9af7993fe2a91620d3fb86a182\n",
        "    \"\"\"\n",
        "    sig = np.asarray(sig)\n",
        "    if sig.dtype.kind != 'f':\n",
        "        raise TypeError(\"'sig' must be a float array\")\n",
        "    dtype = np.dtype(dtype)\n",
        "    if dtype.kind not in 'iu':\n",
        "        raise TypeError(\"'dtype' must be an integer type\")\n",
        "    i = np.iinfo(dtype)\n",
        "    abs_max = 2 ** (i.bits - 1)\n",
        "    offset = i.min + abs_max\n",
        "    return (sig * abs_max + offset).clip(i.min, i.max).astype(dtype)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference(audio, model_tag):\n",
        "  sr, audio = audio\n",
        "  audio = convert_to_16_bit_wav(audio)\n",
        "  audio = pcm2float(audio)\n",
        "  audio = torch.tensor(audio).unsqueeze(0)\n",
        "  audio = torchaudio.transforms.Resample(orig_freq=sr,\n",
        "                                         new_freq=16000)(audio)\n",
        "  print(model_tag, file=sys.stderr)\n",
        "  model = torch.hub.load(\"deep-privacy/SA-toolkit\", \"anonymization\", tag_version=model_tag, trust_repo=True)\n",
        "  model.eval()\n",
        "  wav_conv = model.convert(audio, target=\"6081\") # hard coded target\n",
        "  return 16000, float2pcm(wav_conv.squeeze().cpu().numpy())\n",
        "\n",
        "\n",
        "article = \"<p style='text-align: center'><a href='https://arxiv.org/abs/2308.04455' target='_blank'>PhD thesis: Anonymizing Speech: Evaluating and Designing Speaker Anonymization Techniques</a> | <a href='https://github.com/deep-privacy/SA-toolkit' target='_blank'>Github Repo</a></p>\"\n",
        "\n",
        "\n",
        "with gr.Blocks() as interface:\n",
        "  gr.Markdown(\n",
        "            \"\"\"\n",
        "            # SA-toolkit\n",
        "            Demo: Speaker speech anonymization toolkit in python\n",
        "            \"\"\"\n",
        "        )\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      audio_input = gr.Audio(sources=[\"upload\", \"microphone\"], type=\"numpy\", label=\"File\",\n",
        "                        interactive=True, elem_id=\"melody-input\")\n",
        "      model_tag = gr.Dropdown([\n",
        "                              'hifigan_bn_tdnnf_wav2vec2_vq_48_v1+f0-transformation=quant_16_awgn_2',\n",
        "                              'hifigan_inception_bn_tdnnf_wav2vec2_train_600_vq_48_v1+f0-transformation=quant_16_awgn_2',\n",
        "                              'hifigan_bn_tdnnf_wav2vec2_vq_48_v1',\n",
        "                              'hifigan_bn_tdnnf_wav2vec2_100h_aug_v1',\n",
        "                              'hifigan_bn_tdnnf_600h_aug_v1',\n",
        "                              'hifigan_bn_tdnnf_600h_vq_48_v1',\n",
        "                              'hifigan_bn_tdnnf_100h_vq_64_v1',\n",
        "                              'hifigan_bn_tdnnf_100h_vq_256_v1',\n",
        "                              'hifigan_bn_tdnnf_100h_aug_v1'], type='value',\n",
        "                              value='hifigan_bn_tdnnf_wav2vec2_vq_48_v1',\n",
        "                             label='Model')\n",
        "      with gr.Row():\n",
        "        submit = gr.Button(\"Submit\")\n",
        "    with gr.Column():\n",
        "      audio_output = gr.Audio(label=\"Output\")\n",
        "  submit.click(inference, inputs=[audio_input, model_tag],\n",
        "                 outputs=[audio_output], batch=False)\n",
        "\n",
        "  gr.Examples(examples=[['3853-163249-0000.flac']],\n",
        "                inputs=[audio_input],\n",
        "                outputs=[audio_output])\n",
        "\n",
        "\n",
        "  gr.HTML(article)\n",
        "  interface.launch()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
