{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75524631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Xuankai Chang\n",
    "#  Apache 2.0  (http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "\"\"\"Encoder definition.\"\"\"\n",
    "import contextlib\n",
    "import copy\n",
    "from filelock import FileLock\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def make_pad_mask(lengths, xs=None, length_dim=-1):\n",
    "    \"\"\"Make mask tensor containing indices of padded part.\n",
    "    Args:\n",
    "        lengths (LongTensor or List): Batch of lengths (B,).\n",
    "        xs (Tensor, optional): The reference tensor.\n",
    "            If set, masks will be the same shape as this tensor.\n",
    "        length_dim (int, optional): Dimension indicator of the above tensor.\n",
    "            See the example.\n",
    "    Returns:\n",
    "        Tensor: Mask tensor containing indices of padded part.\n",
    "                dtype=torch.uint8 in PyTorch 1.2-\n",
    "                dtype=torch.bool in PyTorch 1.2+ (including 1.2)\n",
    "    Examples:\n",
    "        With only lengths.\n",
    "        >>> lengths = [5, 3, 2]\n",
    "        >>> make_non_pad_mask(lengths)\n",
    "        masks = [[0, 0, 0, 0 ,0],\n",
    "                 [0, 0, 0, 1, 1],\n",
    "                 [0, 0, 1, 1, 1]]\n",
    "        With the reference tensor.\n",
    "        >>> xs = torch.zeros((3, 2, 4))\n",
    "        >>> make_pad_mask(lengths, xs)\n",
    "        tensor([[[0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0]],\n",
    "                [[0, 0, 0, 1],\n",
    "                 [0, 0, 0, 1]],\n",
    "                [[0, 0, 1, 1],\n",
    "                 [0, 0, 1, 1]]], dtype=torch.uint8)\n",
    "        >>> xs = torch.zeros((3, 2, 6))\n",
    "        >>> make_pad_mask(lengths, xs)\n",
    "        tensor([[[0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 1]],\n",
    "                [[0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 1, 1, 1]],\n",
    "                [[0, 0, 1, 1, 1, 1],\n",
    "                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "        With the reference tensor and dimension indicator.\n",
    "        >>> xs = torch.zeros((3, 6, 6))\n",
    "        >>> make_pad_mask(lengths, xs, 1)\n",
    "        tensor([[[0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [1, 1, 1, 1, 1, 1]],\n",
    "                [[0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [1, 1, 1, 1, 1, 1],\n",
    "                 [1, 1, 1, 1, 1, 1],\n",
    "                 [1, 1, 1, 1, 1, 1]],\n",
    "                [[0, 0, 0, 0, 0, 0],\n",
    "                 [0, 0, 0, 0, 0, 0],\n",
    "                 [1, 1, 1, 1, 1, 1],\n",
    "                 [1, 1, 1, 1, 1, 1],\n",
    "                 [1, 1, 1, 1, 1, 1],\n",
    "                 [1, 1, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "        >>> make_pad_mask(lengths, xs, 2)\n",
    "        tensor([[[0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 1],\n",
    "                 [0, 0, 0, 0, 0, 1]],\n",
    "                [[0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 1, 1, 1],\n",
    "                 [0, 0, 0, 1, 1, 1]],\n",
    "                [[0, 0, 1, 1, 1, 1],\n",
    "                 [0, 0, 1, 1, 1, 1],\n",
    "                 [0, 0, 1, 1, 1, 1],\n",
    "                 [0, 0, 1, 1, 1, 1],\n",
    "                 [0, 0, 1, 1, 1, 1],\n",
    "                 [0, 0, 1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "    \"\"\"\n",
    "    if length_dim == 0:\n",
    "        raise ValueError(\"length_dim cannot be 0: {}\".format(length_dim))\n",
    "\n",
    "    if not isinstance(lengths, list):\n",
    "        lengths = lengths.tolist()\n",
    "    bs = int(len(lengths))\n",
    "    if xs is None:\n",
    "        maxlen = int(max(lengths))\n",
    "    else:\n",
    "        maxlen = xs.size(length_dim)\n",
    "\n",
    "    seq_range = torch.arange(0, maxlen, dtype=torch.int64)\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(bs, maxlen)\n",
    "    seq_length_expand = seq_range_expand.new(lengths).unsqueeze(-1)\n",
    "    mask = seq_range_expand >= seq_length_expand\n",
    "\n",
    "    if xs is not None:\n",
    "        assert xs.size(0) == bs, (xs.size(0), bs)\n",
    "\n",
    "        if length_dim < 0:\n",
    "            length_dim = xs.dim() + length_dim\n",
    "        # ind = (:, None, ..., None, :, , None, ..., None)\n",
    "        ind = tuple(\n",
    "            slice(None) if i in (0, length_dim) else None for i in range(xs.dim())\n",
    "        )\n",
    "        mask = mask[ind].expand_as(xs).to(xs.device)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class LayerNorm(torch.nn.LayerNorm):\n",
    "    \"\"\"Layer normalization module.\n",
    "    Args:\n",
    "        nout (int): Output dim size.\n",
    "        dim (int): Dimension to be normalized.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nout, dim=-1):\n",
    "        \"\"\"Construct an LayerNorm object.\"\"\"\n",
    "        super(LayerNorm, self).__init__(nout, eps=1e-12)\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply layer normalization.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor.\n",
    "        \"\"\"\n",
    "        if self.dim == -1:\n",
    "            return super(LayerNorm, self).forward(x)\n",
    "        return super(LayerNorm, self).forward(x.transpose(1, -1)).transpose(1, -1)\n",
    "    \n",
    "\n",
    "class FairSeqWav2Vec2Encoder(torch.nn.Module):\n",
    "    \"\"\"FairSeq Wav2Vec2 encoder module.\n",
    "\n",
    "    Args:\n",
    "        input_size: input dim\n",
    "        output_size: dimension of attention\n",
    "        w2v_url: url to Wav2Vec2.0 pretrained model\n",
    "        w2v_dir_path: directory to download the Wav2Vec2.0 pretrained model.\n",
    "        normalize_before: whether to use layer_norm before the first block\n",
    "        finetune_last_n_layers: last n layers to be finetuned in Wav2Vec2.0\n",
    "                                0 means to finetune every layer if freeze_w2v=False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        w2v_url: str,\n",
    "        w2v_dir_path: str = \"./\",\n",
    "        output_size: int = 256,\n",
    "        normalize_before: bool = False,\n",
    "        freeze_finetune_updates: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if w2v_url != \"\":\n",
    "            try:\n",
    "                import fairseq\n",
    "                from fairseq.models.wav2vec.wav2vec2 import Wav2Vec2Model\n",
    "            except Exception as e:\n",
    "                print(\"Error: FairSeq is not properly installed.\")\n",
    "                print(\n",
    "                    \"Please install FairSeq: cd ${MAIN_ROOT}/tools && make fairseq.done\"\n",
    "                )\n",
    "                raise e\n",
    "\n",
    "        self.w2v_model_path = download_w2v(w2v_url, w2v_dir_path)\n",
    "\n",
    "        self._output_size = output_size\n",
    "\n",
    "        models, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task(\n",
    "            [self.w2v_model_path],\n",
    "            arg_overrides={\"data\": w2v_dir_path},\n",
    "        )\n",
    "        model = models[0]\n",
    "\n",
    "        if not isinstance(model, Wav2Vec2Model):\n",
    "            try:\n",
    "                model = model.w2v_encoder.w2v_model\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"Error: pretrained models should be within: \"\n",
    "                    \"'Wav2Vec2Model, Wav2VecCTC' classes, etc.\"\n",
    "                )\n",
    "                raise e\n",
    "\n",
    "        self.encoders = model\n",
    "\n",
    "        self.pretrained_params = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        self.normalize_before = normalize_before\n",
    "        if self.normalize_before:\n",
    "            self.after_norm = LayerNorm(output_size)\n",
    "\n",
    "        if model.cfg.encoder_embed_dim != output_size:\n",
    "            # TODO(xkc09): try LSTM\n",
    "            self.output_layer = torch.nn.Sequential(\n",
    "                torch.nn.Linear(model.cfg.encoder_embed_dim, output_size),\n",
    "            )\n",
    "        else:\n",
    "            self.output_layer = None\n",
    "\n",
    "        self.freeze_finetune_updates = freeze_finetune_updates\n",
    "        self.register_buffer(\"num_updates\", torch.LongTensor([0]))\n",
    "\n",
    "    def output_size(self) -> int:\n",
    "        return self._output_size\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        xs_pad: torch.Tensor,\n",
    "        ilens: torch.Tensor,\n",
    "        prev_states: torch.Tensor = None,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"Forward FairSeqWav2Vec2 Encoder.\n",
    "\n",
    "        Args:\n",
    "            xs_pad: input tensor (B, L, D)\n",
    "            ilens: input length (B)\n",
    "            prev_states: Not to be used now.\n",
    "        Returns:\n",
    "            position embedded tensor and mask\n",
    "        \"\"\"\n",
    "        masks = make_pad_mask(ilens).to(xs_pad.device)\n",
    "\n",
    "        ft = self.freeze_finetune_updates <= self.num_updates\n",
    "        if self.num_updates <= self.freeze_finetune_updates:\n",
    "            self.num_updates += 1\n",
    "        elif ft and self.num_updates == self.freeze_finetune_updates + 1:\n",
    "            self.num_updates += 1\n",
    "            logging.info(\"Start fine-tuning wav2vec parameters!\")\n",
    "\n",
    "        with torch.no_grad() if not ft else contextlib.nullcontext():\n",
    "            enc_outputs = self.encoders(\n",
    "                xs_pad,\n",
    "                masks,\n",
    "                features_only=True,\n",
    "            )\n",
    "\n",
    "        xs_pad = enc_outputs[\"x\"]  # (B,T,C),\n",
    "        masks = enc_outputs[\"padding_mask\"]  # (B, T)\n",
    "\n",
    "        olens = (~masks).sum(dim=1)\n",
    "\n",
    "        if self.output_layer is not None:\n",
    "            xs_pad = self.output_layer(xs_pad)\n",
    "\n",
    "        if self.normalize_before:\n",
    "            xs_pad = self.after_norm(xs_pad)\n",
    "\n",
    "        return xs_pad, olens, None\n",
    "\n",
    "    def reload_pretrained_parameters(self):\n",
    "        self.encoders.load_state_dict(self.pretrained_params)\n",
    "        logging.info(\"Pretrained Wav2Vec model parameters reloaded!\")\n",
    "\n",
    "\n",
    "def download_w2v(model_url, dir_path):\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    model_name = model_url.split(\"/\")[-1]\n",
    "    model_path = os.path.join(dir_path, model_name)\n",
    "\n",
    "    dict_url = \"https://dl.fbaipublicfiles.com/fairseq/wav2vec/dict.ltr.txt\"\n",
    "    dict_path = os.path.join(dir_path, dict_url.split(\"/\")[-1])\n",
    "\n",
    "    with FileLock(model_path + \".lock\"):\n",
    "        if not os.path.exists(model_path):\n",
    "            torch.hub.download_url_to_file(model_url, model_path)\n",
    "            torch.hub.download_url_to_file(dict_url, dict_path)\n",
    "            logging.info(f\"Wav2Vec model downloaded {model_path}\")\n",
    "        else:\n",
    "            logging.info(f\"Wav2Vec model {model_path} already exists.\")\n",
    "\n",
    "    return model_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35b38e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec2 = FairSeqWav2Vec2Encoder(input_size=-1, w2v_url=\"https://dl.fbaipublicfiles.com/fairseq/wav2vec/wav2vec_small_960h.pt\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8060da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:142: operator(): block: [0,0,0], thread: [0,0,0] Assertion `index >= -sizes[i] && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9528/3447650059.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 1s speech with sampling rate 16KHz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_lens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m34000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav2vec2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/asr-based-privacy-preserving-separation/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9528/685715676.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs_pad, ilens, prev_states)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mft\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             enc_outputs = self.encoders(\n\u001b[0m\u001b[1;32m    249\u001b[0m                 \u001b[0mxs_pad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/asr-based-privacy-preserving-separation/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lium/raid01_b/pchampi/lab/asr-based-privacy-preserving-separation/fairseq/fairseq/models/wav2vec/wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, padding_mask, mask, features_only, layer, mask_indices, mask_channel_indices, padding_count)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_extract_proj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_extract_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/asr-based-privacy-preserving-separation/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/asr-based-privacy-preserving-separation/venv/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/lab/asr-based-privacy-preserving-separation/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 33000, requires_grad=True).cuda()  # 1s speech with sampling rate 16KHz\n",
    "x_lens = torch.LongTensor([34000, 16000]).cuda()\n",
    "a, b, _ = wav2vec2(x, x_lens)\n",
    "print(a.shape, b.shape, x[0].shape[0]/a.shape[1])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for i in range(16000, 16000*3, 1):\n",
    "#         x = torch.randn(2, i, requires_grad=True).cuda()  # 1s speech with sampling rate 16KHz\n",
    "#         x_lens = torch.LongTensor([i, 15000]).cuda()\n",
    "#         a, b, _ = wav2vec2(x, x_lens)\n",
    "#         print(a.shape, b.shape, x[0].shape[0]/a.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e457c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09e9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
